<!DOCTYPE html>
<!-- saved from url=(0043)http://localhost:3000/#proprietary-workflow -->
<html lang="en" class="dark" style="scroll-padding:0"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Performing Object Detection on Drone Orthomosaics with Meta's Segment Anything Model (SAM)</title><meta property="og:title" content="Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)"><meta name="generator" content="mystmd"><meta name="description" content="This article presents a workflow that utilizes SAM&#39;s automatic mask generation skill to effectively perform the task of object detection zero-shot on a high-resolution drone orthomosaic. The generated output is 20% more spatially accurate than that produced using proprietary software, with 400% greater IoU."><meta property="og:description" content="This article presents a workflow that utilizes SAM&#39;s automatic mask generation skill to effectively perform the task of object detection zero-shot on a high-resolution drone orthomosaic. The generated output is 20% more spatially accurate than that produced using proprietary software, with 400% greater IoU."><meta name="keywords" content="object detection, spatial localization, remote sensing"><meta name="image" content="http://localhost:3100/trimmer-cc850f3b5f2eb92884f6511d6db638c6.jpg"><meta property="og:image" content="http://localhost:3100/trimmer-cc850f3b5f2eb92884f6511d6db638c6.jpg"><link rel="stylesheet" href="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/app-QQ3IDNE5.css"><link rel="stylesheet" href="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/thebe-core-VKVHG5VY.css"><link rel="stylesheet" href="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/mpl_widget.css"><link rel="stylesheet" href="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/font-awesome.css"><link rel="stylesheet" href="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"><link rel="icon" href="http://localhost:3000/favicon.ico"><link rel="stylesheet" href="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/myst-theme.css"><script src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/thebe-core.min.js.download" async="true" type="text/javascript"></script></head><body class="m-0 transition-colors duration-500 bg-white dark:bg-stone-900"><div class="fixed top-1 left-1 h-[0px] w-[0px] focus-within:z-40 focus-within:h-auto focus-within:w-auto bg-white overflow-hidden focus-within:p-2 focus-within:ring-1" aria-label="skip to content options"><a href="http://localhost:3000/#skip-to-article" class="block px-2 py-1 text-black underline">Skip to article content</a></div><div class="fixed top-4 right-4 z-50"><button class="theme rounded-full aspect-square border border-stone-700 dark:border-white hover:bg-neutral-100 border-solid overflow-hidden text-stone-700 dark:text-white hover:text-stone-500 dark:hover:text-neutral-800 w-8 h-8 mx-3" title="Toggle theme between light and dark mode." aria-label="Toggle theme between light and dark mode."><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 hidden dark:block"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 0 1 .162.819A8.97 8.97 0 0 0 9 6a9 9 0 0 0 9 9 8.97 8.97 0 0 0 3.463-.69.75.75 0 0 1 .981.98 10.503 10.503 0 0 1-9.694 6.46c-5.799 0-10.5-4.7-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 0 1 .818.162Z" clip-rule="evenodd"></path></svg><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-full w-full p-0.5 dark:hidden"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 1 1-7.5 0 3.75 3.75 0 0 1 7.5 0Z"></path></svg></button></div><article class="article content article-left-grid subgrid-gap"><div class="hidden"></div><header class="relative col-screen"><div class="absolute article-left-grid subgrid-gap col-screen bg-no-repeat bg-cover bg-top w-full h-full -z-10 pointer-events-none" style="background-image:url(http://localhost:3100/banner-757ff4dfc8e2a2f9366ba20f5f57dfeb.png)"></div><div class="w-full relative col-screen article article-left-grid subgrid-gap my-[2rem] pb-[1rem] md:my-[4rem]"><div class="col-page-right shadow-2xl bg-white/80 dark:bg-black/80 backdrop-blur"><div class="flex w-full align-middle py-2 mb-[1rem] text-sm px-4 w-full bg-white/80 dark:bg-black/80 col-page-right"><div class="flex-none pr-2 smallcaps">Research Article</div><div class="flex-none mr-2 hidden pl-2 border-l md:block"><a href="https://proceedings.scipy.org/" class="font-semibold no-underline smallcaps" title="Python in Science Conference">Python in Science Conference</a></div><div class="flex-grow"></div><div class="hidden sm:block"><a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer" class="opacity-50 hover:opacity-100 text-inherit hover:text-inherit" aria-label="Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mx-1"><title>Content License: Creative Commons Attribution 4.0 International (CC-BY-4.0)</title><path d="M12 2.2c2.7 0 5 1 7 2.9.9.9 1.6 2 2.1 3.1.5 1.2.7 2.4.7 3.8 0 1.3-.2 2.6-.7 3.8-.5 1.2-1.2 2.2-2.1 3.1-1 .9-2 1.7-3.2 2.2-1.2.5-2.5.7-3.7.7s-2.6-.3-3.8-.8c-1.2-.5-2.2-1.2-3.2-2.1s-1.6-2-2.1-3.2-.8-2.4-.8-3.7c0-1.3.2-2.5.7-3.7S4.2 6 5.1 5.1C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C5.6 7.1 5 8 4.6 9c-.4 1-.6 2-.6 3s.2 2.1.6 3c.4 1 1 1.8 1.8 2.6S8 19 9 19.4c1 .4 2 .6 3 .6s2.1-.2 3-.6c1-.4 1.9-1 2.7-1.8 1.5-1.5 2.3-3.3 2.3-5.6 0-1.1-.2-2.1-.6-3.1-.4-1-1-1.8-1.7-2.6C16.1 4.8 14.2 4 12 4zm-.1 6.4l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.5.3-1 .4-1.5.4-.9 0-1.6-.3-2.1-.8-.5-.6-.8-1.3-.8-2.3 0-.9.3-1.7.8-2.2.6-.6 1.3-.8 2.1-.8 1.2 0 2.1.4 2.6 1.4zm5.6 0l-1.3.7c-.1-.3-.3-.5-.5-.6-.2-.1-.4-.2-.6-.2-.9 0-1.3.6-1.3 1.7 0 .5.1.9.3 1.3.2.3.5.5 1 .5.6 0 1-.3 1.2-.8l1.2.6c-.3.5-.6.9-1.1 1.1-.4.2-.9.3-1.4.3-.9 0-1.6-.3-2.1-.8s-.8-1.3-.8-2.2c0-.9.3-1.7.8-2.2.5-.5 1.2-.8 2-.8 1.2 0 2.1.4 2.6 1.4z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="inline-block mr-1"><title>Credit must be given to the creator</title><path d="M12 2.2c2.7 0 5 .9 6.9 2.8 1.9 1.9 2.8 4.2 2.8 6.9s-.9 5-2.8 6.8c-2 1.9-4.3 2.9-7 2.9-2.6 0-4.9-1-6.9-2.9-1.8-1.7-2.8-4-2.8-6.7s1-5 2.9-6.9C7 3.2 9.3 2.2 12 2.2zM12 4c-2.2 0-4.1.8-5.6 2.3C4.8 8 4 9.9 4 12c0 2.2.8 4 2.4 5.6C8 19.2 9.8 20 12 20c2.2 0 4.1-.8 5.7-2.4 1.5-1.5 2.3-3.3 2.3-5.6 0-2.2-.8-4.1-2.3-5.7C16.1 4.8 14.2 4 12 4zm2.6 5.6v4h-1.1v4.7h-3v-4.7H9.4v-4c0-.2.1-.3.2-.4.1-.2.2-.2.4-.2h4c.2 0 .3.1.4.2.2.1.2.2.2.4zm-4-2.5c0-.9.5-1.4 1.4-1.4s1.4.5 1.4 1.4c0 .9-.5 1.4-1.4 1.4s-1.4-.5-1.4-1.4z"></path></svg></a><a href="https://en.wikipedia.org/wiki/Open_access" target="_blank" rel="noopener noreferrer" title="Open Access" class="text-inherit hover:text-inherit"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1.25rem" height="1.25rem" class="mr-1 inline-block opacity-60 hover:opacity-100 hover:text-[#E18435]"><path d="M17.1 12.6h-2V7.5c0-1.7-1.4-3.1-3-3.1-.8 0-1.6.3-2.2.9-.6.5-.9 1.3-.9 2.2v.7H7v-.7c0-1.4.5-2.7 1.5-3.7s2.2-1.5 3.6-1.5 2.6.5 3.6 1.5 1.5 2.3 1.5 3.7v5.1z"></path><path d="M12 21.8c-.8 0-1.6-.2-2.3-.5-.7-.3-1.4-.8-1.9-1.3-.6-.6-1-1.2-1.3-2-.3-.8-.5-1.6-.5-2.4s.2-1.6.5-2.4c.3-.7.7-1.4 1.3-2s1.2-1 1.9-1.3c.7-.3 1.5-.5 2.3-.5.8 0 1.6.2 2.3.5.7.3 1.4.8 1.9 1.3.6.6 1 1.2 1.3 2 .3.8.5 1.6.5 2.4s-.2 1.6-.5 2.4c-.3.7-.7 1.4-1.3 2-.6.6-1.2 1-1.9 1.3-.7.3-1.5.5-2.3.5zm0-10.3c-2.2 0-4 1.8-4 4.1s1.8 4.1 4 4.1 4-1.8 4-4.1-1.8-4.1-4-4.1z"></path><circle cx="12" cy="15.6" r="1.7"></circle></svg></a></div></div><div class="flex flex-col mb-10 md:flex-row"><div id="skip-to-frontmatter" aria-label="article frontmatter" class="flex-grow pt-6 px-6 col-body"><h1 class="mb-0">Performing Object Detection on Drone Orthomosaics with Meta's Segment Anything Model (SAM)</h1><div><span class="font-semibold text-sm inline-block"><button class="focus:shadow-[0_0_0_2px] focus:shadow-black outline-none hover:underline" aria-label="Author Details" type="button" aria-haspopup="dialog" aria-expanded="false" aria-controls="radix-:R1oqf8p:" data-state="closed">Nicholas McCarty</button><a class="ml-1" href="mailto:nick@upskilled.consulting" title="Nicholas McCarty &lt;nick@upskilled.consulting&gt;" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1rem" height="1rem" class="inline-block text-gray-400 hover:text-blue-400 -translate-y-[0.1em]"><path d="M21.8 18c0 1.1-.9 2-1.9 2H4.2c-1.1 0-1.9-.9-1.9-2V9.9c0-.5.3-.7.8-.4l7.8 4.7c.7.4 1.7.4 2.4 0L21 9.5c.4-.2.8-.1.8.4V18z"></path><path d="M21.8 6c0-1.1-.9-2-1.9-2H4.2c-1.1 0-2 .9-2 2v.4c0 .5.3 1.1.8 1.3l8.5 5.1c.2.1.7.1.9 0l8.6-5c.4-.3.8-.9.8-1.3-.1-.1-.1-.5 0-.5z"></path></svg></a><a class="ml-1" href="https://orcid.org/0009-0001-3727-9178" target="_blank" rel="noopener noreferrer" title="ORCID (Open Researcher and Contributor ID)"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="1rem" height="1rem" class="inline-block text-gray-400 hover:text-[#A9C751] -translate-y-[0.1em]"><path d="M21.8 12c0 5.4-4.4 9.8-9.8 9.8S2.2 17.4 2.2 12 6.6 2.2 12 2.2s9.8 4.4 9.8 9.8zM8.2 5.8c-.4 0-.8.3-.8.8s.3.8.8.8.8-.4.8-.8-.3-.8-.8-.8zm2.3 9.6h1.2v-6h1.8c2.3 0 3.3 1.4 3.3 3s-1.5 3-3.3 3h-3v1.1H9V8.3H7.7v8.2h5.9c3.3 0 4.5-2.2 4.5-4.1s-1.2-4.1-4.3-4.1h-3.2l-.1 7.1z"></path></svg></a></span></div><div class="flex mt-2 text-sm font-light"><time datetime="2025-07-10" class="">July 10, 2025</time></div></div><div class="pt-5 md:self-center h-fit lg:pt-0 col-body lg:col-margin-right-inset"></div></div></div></div></header><main id="main" class="article-left-grid subgrid-gap col-screen pt-10"><div class="hidden"></div><div class="block my-10 lg:sticky lg:top-0 lg:z-10 lg:h-0 lg:pt-0 lg:my-0 lg:ml-10 lg:col-margin-right" style="top:24px"><div data-state="open"><nav aria-label="Document Outline" class="not-prose overflow-y-auto transition-opacity duration-700 relative pt-[2px]" style="top: 0px; max-height: calc(-20px + 100vh);"><div class="flex flex-row gap-2 mb-4 text-sm leading-6 uppercase rounded-lg text-slate-900 dark:text-slate-100">In this article<button class="self-center flex-none rounded-md group hover:bg-slate-300/30 focus:outline outline-blue-200 outline-2" type="button" aria-controls="radix-:r0:" aria-expanded="true" data-state="open"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="transition-transform duration-300 group-data-[state=open]:rotate-90 text-text-slate-700 dark:text-slate-100" height="1.5rem" width="1.5rem"><path fill-rule="evenodd" d="M16.28 11.47a.75.75 0 0 1 0 1.06l-7.5 7.5a.75.75 0 0 1-1.06-1.06L14.69 12 7.72 5.03a.75.75 0 0 1 1.06-1.06l7.5 7.5Z" clip-rule="evenodd"></path></svg></button></div><div data-state="open" id="radix-:r0:" class="CollapsibleContent" style="transition-duration: 0s; animation-name: none; --radix-collapsible-content-height: 536px; --radix-collapsible-content-width: 403.9312438964844px;"><ul class="text-sm leading-6 text-slate-400"><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#introduction">Introduction</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#motivation">Motivation</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#approach">Approach</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#methodology">Methodology</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-4" href="http://localhost:3000/#data-and-environment">Data and Environment</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-4" href="http://localhost:3000/#workflow">Workflow</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-8 text-xs" href="http://localhost:3000/#data-ingestion-and-preprocessing">Data Ingestion and Preprocessing</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-8 text-xs" href="http://localhost:3000/#mask-generation">Mask Generation</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-8 text-xs" href="http://localhost:3000/#post-processing">Post-Processing</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-8 text-xs" href="http://localhost:3000/#accuracy-evaluation">Accuracy Evaluation</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-8 text-xs" href="http://localhost:3000/#benchmarking">Benchmarking</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#results">Results</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-4" href="http://localhost:3000/#proprietary-workflow">Proprietary Workflow</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-500 dark:text-slate-300 pr-2 pl-4" href="http://localhost:3000/#open-source-workflow">Open Source Workflow</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#discussion">Discussion</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#conclusion">Conclusion</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#conflicts-of-interest">Conflicts of Interest</a></li><li class="border-l-2 hover:border-l-blue-500 border-l-gray-300 dark:border-l-gray-50"><a class="block p-1 text-slate-900 dark:text-slate-50 pr-2 pl-2" href="http://localhost:3000/#ai-usage-disclosure">AI Usage Disclosure</a></li></ul></div></nav></div></div><div id="skip-to-article"></div><div><div><h2 id="abstract" class="mb-3 text-base font-semibold group">Abstract<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#abstract" title="Link to Abstract" aria-label="Link to Abstract">¶</a></h2><div class="px-6 py-1 mb-3 rounded-sm bg-slate-50 dark:bg-slate-800"><div class="col-body"><p>Accurate and efficient object detection and spatial localization in remote sensing imagery is a persistent challenge. In the context of precision agriculture, the extensive data annotation required by conventional deep learning models poses additional challenges. This paper presents a fully open source workflow leveraging Meta AI’s Segment Anything Model (SAM) for zero-shot segmentation, enabling scalable object detection and spatial localization in high-resolution drone orthomosaics without the need for annotated image datasets. Model training and/or fine-tuning is rendered unnecessary in our precision agriculture-focused use case. The presented end-to-end workflow takes high-resolution images and quality control (QC) check points as inputs, automatically generates masks corresponding to the objects of interest (empty plant pots, in our given context), and outputs their spatial locations in real-world coordinates. Detection accuracy (required in the given context to be within 3 cm) is then quantitatively evaluated using the ground truth QC check points and benchmarked against object detection output generated using proprietary software. Results demonstrate that the open source workflow achieves superior spatial accuracy — producing output <code>20% more spatially accurate</code> than that produced using the proprietary software, with <code>400% greater IoU</code> — while providing a scalable way to perform spatial localization on high-resolution imagery (with ground sampling distance, or GSD, &lt; 30 cm).</p></div></div></div><div class="mb-10 group"><span class="mr-2 font-semibold">Keywords:</span><span class="after:content-[&#39;,&#39;] after:mr-1">object detection</span><span class="after:content-[&#39;,&#39;] after:mr-1">spatial localization</span><span class="">remote sensing</span><a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#keywords" title="Link to Keywords" aria-label="Link to Keywords">¶</a></div></div><h2 id="introduction" class="relative group"><span class="mr-3 select-none">1</span><span class="heading-text">Introduction</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#introduction" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Image segmentation is a critical task in geospatial analysis, enabling the identification and extraction of relevant features from high resolution remote sensing imagery <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.21105/joss.05663" target="_blank" rel="noreferrer" class="hover-link">Wu &amp; Osco, 2023</a></cite></span>. However, extracting actionable information (i.e., object detection and spatial localization) can be constrained by the need for large, labeled datasets to train deep learning models in order to then perform inference and (hopefully) produce the desired output. This bottleneck is particularly acute in agricultural domains, where variability in conditions and object types complicates manual annotation <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://arxiv.org/abs/2306.16623" target="_blank" rel="noreferrer" class="hover-link">Osco <em>et al.</em>, 2023</a></cite></span>.</p><p>Recent advances in foundation models, such as Meta AI’s Segment Anything Model (SAM), offer a promising path forward. SAM is designed for promptable “zero-shot” segmentation. “Prompt engineering”, in this context, involves using points and bounding boxes to focus the model’s efforts on more efficiently generating masks corresponding to objects of interest <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://arxiv.org/abs/2310.01845" target="_blank" rel="noreferrer" class="hover-link">Mayladan <em>et al.</em>, 2024</a></cite></span>. Providing these prompts allows accurate masks to be generated for novel objects (ones not included in SAM’s training corpus), without domain-specific training. Masks can also be generated automatically with no such prompting. SAM’s automatic mask generator will effectively “detect” everything using open source model checkpoints and generate masks for each object in a provided image <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://arxiv.org/abs/2304.02643" target="_blank" rel="noreferrer" class="hover-link">Kirillov <em>et al.</em>, 2023</a></cite></span>.</p><p>While SAM’s ability to generalize is impressive <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://arxiv.org/abs/2304.02643" target="_blank" rel="noreferrer" class="hover-link">Kirillov <em>et al.</em>, 2023</a></cite><cite class="" data-state="closed"><a href="https://arxiv.org/abs/2306.16623" target="_blank" rel="noreferrer" class="hover-link">Osco <em>et al.</em>, 2023</a></cite></span>, its performance on remote sensing imagery and fine-grained features requires careful workflow integration and evaluation <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.21105/joss.05663" target="_blank" rel="noreferrer" class="hover-link">Wu &amp; Osco, 2023</a></cite></span>. This paper describes a comprehensive, open source workflow for object detection and spatial localization in remote sensing imagery imagery, built around SAM and widely used geospatial Python libraries <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.5281/zenodo.5884351" target="_blank" rel="noreferrer" class="hover-link">GDAL/OGR contributors, 2025</a></cite><cite class="" data-state="closed"><a href="https://doi.org/10.5281/zenodo.5597138" target="_blank" rel="noreferrer" class="hover-link">Gillies <em>et al.</em>, 2025</a></cite><cite class="" data-state="closed"><a href="https://doi.org/10.5281/zenodo.3946761" target="_blank" rel="noreferrer" class="hover-link">Jordahl <em>et al.</em>, 2020</a></cite><cite class="" data-state="closed"><a href="https://github.com/rasterio/rasterio" target="_blank" rel="noreferrer" class="hover-link">Gillies &amp; others, 2013</a></cite></span>. The complete process is delineated, from data loading and preprocessing to mask generation, post-processing, and quantitative accuracy assessment, culminating in a robust comparison with results produced using a proprietary software solution.</p><h2 id="motivation" class="relative group"><span class="mr-3 select-none">2</span><span class="heading-text">Motivation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#motivation" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Precision agriculture relies on accurate object detection for tasks such as plant counting, health monitoring, and targeted resource distribution. Traditional deep learning approaches can become hindered by the cost and effort of generating carefully annotated data, limiting scalability and accessibility. Proprietary solutions, while effective, can be expensive and opaque, impeding reproducibility and customization.</p><figure id="fig-trimmer" class="fig-figure"><img id="khnX9TO2Rw" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/trimmer-cc850f3b5f2eb92884f6511d6db638c6.jpg" alt="The derived centroids of the objects detected in the drone orthomosaic are used to automate this nursery trimmer." data-canonical-url="trimmer.jpg"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-trimmer" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->1<!-- -->:</a>The derived centroids of the objects detected in the drone orthomosaic are used to automate this nursery trimmer.</p></figcaption></figure><p>SAM’s zero-shot segmentation capability directly addresses the data annotation bottleneck, enabling rapid deployment in novel contexts. By developing an open source workflow around SAM, an end-to-end pipeline is created which allows for the quantitative evaluation of spatial accuracy with respect to objects detected in high-resolution aerial imagery. This modular workflow can also be repurposed as an automated data annotation pipeline for downstream model training/fine-tuning, if required.</p><h2 id="approach" class="relative group"><span class="mr-3 select-none">3</span><span class="heading-text">Approach</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#approach" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>Our approach integrates SAM’s segmentation strengths with traditional geospatial data processing techniques, which lends itself to our precision agriculture use case. The workflow, like any other, can be thought of as a sequence of steps (visualized above and described below), each with their own sets of substeps:</p><ul><li><strong>Data Ingestion</strong>: Loading GeoTIFF orthomosaics and QC point CSVs, extracting spatial bounds and coordinate reference systems (CRS) using Rasterio or GDAL.</li><li><strong>Preprocessing</strong>: Filtering QC points to those within image bounds, standardizing coordinate columns, and saving filtered data for downstream analysis.</li><li><strong>Mask Generation</strong>: Tiling large images for efficient processing, running SAM’s automatic mask generator (<strong><em>ViT-H</em></strong> variant) on each tile, and filtering masks by confidence.</li><li><strong>Post-Processing</strong>: Converting masks to polygons, filtering by area and compactness, merging overlapping geometry, and extracting centroids.</li><li><strong>Accuracy Evaluation</strong>: Calculating point-to-centroid deviations (in centimeters) between detected objects and QC points, compiling results, and generating visual and tabular reports.</li><li><strong>Benchmarking</strong>: Quantitatively comparing SAM-based results against the proprietary solution using identical evaluation metrics.</li></ul><p>It should be noted that there are no model training or fine-tuning steps included in our workflow, as we are using a foundation model to generate masks. This is analogous to using ChatGPT to generate text, which does not require users to train or fine-tune the underlying foundation model in order to do so.</p><p>This approach is carried out entirely using open source Python libraries, ensuring transparency and extensibility.</p><h2 id="methodology" class="relative group"><span class="mr-3 select-none">4</span><span class="heading-text">Methodology</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#methodology" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><h3 id="data-and-environment" class="relative group"><span class="mr-3 select-none">4.1</span><span class="heading-text">Data and Environment</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#data-and-environment" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><ul><li><strong>Imagery</strong>: High-resolution GeoTIFF orthomosaic<ul><li><strong>Image size</strong>: 18,200 x 55,708; 1,013,885,600 pixels</li><li><strong>Total area</strong>: 545,243 sq ft; 12.5 acres</li><li><strong>GSD</strong>: 0.71 cm</li></ul></li><li><strong>Ground Truth</strong>: QC points in CSV format, containing spatial coordinates and unique identifiers.</li><li><strong>Coordinate Reference System (CRS) Transformations</strong>: All spatial operations are performed using the NAD83 CRS (EPSG:6859), with reprojection to the WGS84 CRS (EPSG:4326) for downstream reporting and nursery trimmer automation.</li><li><strong>Dependencies</strong><span id="fnref-i3t457uC19" class="" data-state="closed"><sup class="hover-link"><a class="no-underline text-inherit hover:text-inherit font-normal hover:underline" href="http://localhost:3000/#fn-footnote-1" title="Link to Footnote" aria-label="Link to Footnote">[<!-- -->1<!-- -->]</a></sup></span>:<ul><li>GDAL <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.5281/zenodo.5884351" target="_blank" rel="noreferrer" class="hover-link">GDAL/OGR contributors, 2025</a></cite></span></li><li>GeoPandas <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.5281/zenodo.3946761" target="_blank" rel="noreferrer" class="hover-link">Jordahl <em>et al.</em>, 2020</a></cite></span></li><li>Matplotlib <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.1109/MCSE.2007.55" target="_blank" rel="noreferrer" class="hover-link">Hunter, 2007</a></cite></span></li><li>NumPy <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.1038/s41586-020-2649-2" target="_blank" rel="noreferrer" class="hover-link">Harris <em>et al.</em>, 2020</a></cite></span></li><li>OpenCV <span class="cite-group parenthetical"><cite class="" data-state="closed"><span class="hover-link">Bradski, 2000</span></cite></span></li><li>OpenPyXL <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://openpyxl.readthedocs.io/" target="_blank" rel="noreferrer" class="hover-link">Gazoni &amp; Clark, 2024</a></cite></span></li><li>Pandas <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.5281/zenodo.3509134" target="_blank" rel="noreferrer" class="hover-link">The Pandas Development Team, 2020</a></cite><cite class="" data-state="closed"><a href="https://doi.org/10.25080/Majora-92bf1922-00a" target="_blank" rel="noreferrer" class="hover-link">McKinney, 2010</a></cite></span></li><li>Pillow <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://buildmedia.readthedocs.org/media/pdf/pillow/latest/pillow.pdf" target="_blank" rel="noreferrer" class="hover-link">Clark, 2015</a></cite></span></li><li>Rasterio <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://github.com/rasterio/rasterio" target="_blank" rel="noreferrer" class="hover-link">Gillies &amp; others, 2013</a></cite></span></li><li>Segment Anything <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://arxiv.org/abs/2304.02643" target="_blank" rel="noreferrer" class="hover-link">Kirillov <em>et al.</em>, 2023</a></cite></span></li><li>Shapely <span class="cite-group parenthetical"><cite class="" data-state="closed"><a href="https://doi.org/10.5281/zenodo.5597138" target="_blank" rel="noreferrer" class="hover-link">Gillies <em>et al.</em>, 2025</a></cite></span></li><li>Torch (<cite class="" data-state="closed"><a href="https://arxiv.org/abs/1912.01703" target="_blank" rel="noreferrer" class="hover-link">Paszke <em>et al.</em> (2019)</a></cite>)</li></ul></li></ul><h3 id="workflow" class="relative group"><span class="mr-3 select-none">4.2</span><span class="heading-text">Workflow</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#workflow" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><figure id="fig-workflow" class="fig-figure"><img id="WAcEzRtbzD" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/workflow-af7b467f0709fce0583142963a37004f.png" alt="The high-level workflow steps." data-canonical-url="workflow.png"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-workflow" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->2<!-- -->:</a>The high-level workflow steps.</p></figcaption></figure><h4 id="data-ingestion-and-preprocessing" class="relative group"><span class="mr-3 select-none">4.2.1</span><span class="heading-text">Data Ingestion and Preprocessing</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#data-ingestion-and-preprocessing" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><figure id="fig-ingestion-and-preprocessing" class="fig-figure"><img id="GDkazdFxcK" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/ingestion-and-prepro-5dd37aadcfd09656ee0ff07de3b097af.png" alt="Data ingestion and preprocessing workflow substeps." data-canonical-url="ingestion-and-preprocessing.png"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-ingestion-and-preprocessing" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->3<!-- -->:</a>Data ingestion and preprocessing workflow substeps.</p></figcaption></figure><h4 id="mask-generation" class="relative group"><span class="mr-3 select-none">4.2.2</span><span class="heading-text">Mask Generation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#mask-generation" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><figure id="fig-mask-generation" class="fig-figure"><img id="whHMIcODvR" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/mask-generation-6063d070d6220423919c6d106fec241c.png" alt="Mask generation workflow substeps." data-canonical-url="mask-generation.png"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-mask-generation" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->4<!-- -->:</a>Mask generation workflow substeps.</p></figcaption></figure><h4 id="post-processing" class="relative group"><span class="mr-3 select-none">4.2.3</span><span class="heading-text">Post-Processing</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#post-processing" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><figure id="fig-post-processing" class="fig-figure"><img id="z2Q92W57YX" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/post-processing-64beff64c6b64b117d7e944816b3c683.png" alt="Data post-processing workflow substeps." data-canonical-url="post-processing.png"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-post-processing" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->5<!-- -->:</a>Data post-processing workflow substeps.</p></figcaption></figure><h4 id="accuracy-evaluation" class="relative group"><span class="mr-3 select-none">4.2.4</span><span class="heading-text">Accuracy Evaluation</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#accuracy-evaluation" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><figure id="fig-accuracy-evaluation" class="fig-figure"><img id="gLkg4ynRRl" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/accuracy-evaluation-fa62865b14e4cead4d16dc102803fc49.png" alt="Data post-processing workflow substeps." data-canonical-url="accuracy-evaluation.png"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-accuracy-evaluation" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->6<!-- -->:</a>Data post-processing workflow substeps.</p></figcaption></figure><h4 id="benchmarking" class="relative group"><span class="mr-3 select-none">4.2.5</span><span class="heading-text">Benchmarking</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#benchmarking" title="Link to this Section" aria-label="Link to this Section">¶</a></h4><figure id="fig-benchmarking" class="fig-figure"><img id="qBjyMaMrIH" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/benchmarking-77f425bca54c3b7d70477a20d4a051ba.png" alt="Benchmarking workflow substeps." data-canonical-url="benchmarking.png"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-benchmarking" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->7<!-- -->:</a>Benchmarking workflow substeps.</p></figcaption></figure><h2 id="results" class="relative group"><span class="mr-3 select-none">5</span><span class="heading-text">Results</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#results" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><figure id="tbl-performance-comparison" class="fig-table"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#tbl-performance-comparison" title="Link to this table" aria-label="Link to this table">Table&nbsp;<!-- -->1<!-- -->:</a>Performance Comparison</p></figcaption><table class=""><tbody><tr class=""><th class="text-center" rowspan="2">Workflow</th><th class="text-center" colspan="3">Detection Quality Metrics</th><th class="text-center" colspan="2">Localization Accuracy Metrics</th></tr><tr class=""><th class="text-center">Precision</th><th class="text-center">Recall</th><th class="text-center">F1 Score</th><th class="text-center">Mean Deviation (cm)</th><th class="text-center">IoU</th></tr><tr class=""><td class="text-center"><strong><em>Proprietary</em></strong></td><td class="text-center">0.9990</td><td class="text-center">1.0000</td><td class="text-center">0.9995</td><td class="text-center">1.39</td><td class="text-center">0.18</td></tr><tr class=""><td class="text-center"><strong><em>Open Source</em></strong></td><td class="text-center">0.9990</td><td class="text-center">0.9965</td><td class="text-center">0.9978</td><td class="text-center">1.20</td><td class="text-center">0.74</td></tr></tbody></table></figure><h3 id="proprietary-workflow" class="relative group"><span class="mr-3 select-none">5.1</span><span class="heading-text">Proprietary Workflow</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#proprietary-workflow" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><figure id="fig-proprietary-workflow-output-evaluation" class="fig-figure"><img id="IAlN087idP" style="margin:0 auto" src="./Performing Object Detection on Drone Orthomosaics with Meta&#39;s Segment Anything Model (SAM)_files/proprietary-workflow-ee60ae2669b98b1bb550af791cfc12d6.png" alt="18 false positives (FP) and hundreds of sliver polygons were observed in the output produced using the proprietary software." data-canonical-url="proprietary-workflow-output-evaluation.png"><figcaption class="group"><p><a class="no-underline text-inherit hover:text-inherit mr-1 font-semibold text-inherit hover:text-inherit hover:font-semibold select-none hover:underline" href="http://localhost:3000/#fig-proprietary-workflow-output-evaluation" title="Link to this figure" aria-label="Link to this figure">Figure&nbsp;<!-- -->8<!-- -->:</a>18 false positives (FP) and hundreds of sliver polygons were observed in the output produced using the proprietary software.</p></figcaption></figure><h3 id="open-source-workflow" class="relative group"><span class="mr-3 select-none">5.2</span><span class="heading-text">Open Source Workflow</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#open-source-workflow" title="Link to this Section" aria-label="Link to this Section">¶</a></h3><h2 id="discussion" class="relative group"><span class="mr-3 select-none">6</span><span class="heading-text">Discussion</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#discussion" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><h2 id="conclusion" class="relative group"><span class="mr-3 select-none">7</span><span class="heading-text">Conclusion</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#conclusion" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>We present a robust, open source workflow for object detection and spatial localization in high-resolution drone orthomosaics, leveraging SAM’s zero-shot segmentation capabilities. Our quantitative evaluation demonstrates improved accuracy over a proprietary software solution, underscoring the potential of foundation models and open source tools to advance scalable, cost-effective feature extraction in agriculture. This work provides a template for further research and deployment in diverse contexts.</p><p>To our knowledge, this is the first comparative evaluation of an open source segmentation model (SAM) against proprietary software in a context requiring high (&lt; 3 cm) spatial accuracy. Our results demonstrate that the workflow not only matches but in some cases exceeds performance metrics with respect to the evaluated output.</p><h2 id="conflicts-of-interest" class="relative group"><span class="mr-3 select-none">9</span><span class="heading-text">Conflicts of Interest</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#conflicts-of-interest" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>The author declares no conflicts of interest.</p><h2 id="ai-usage-disclosure" class="relative group"><span class="mr-3 select-none">10</span><span class="heading-text">AI Usage Disclosure</span><a class="no-underline text-inherit hover:text-inherit inline-block w-0 px-0 translate-x-[10px] font-normal select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#ai-usage-disclosure" title="Link to this Section" aria-label="Link to this Section">¶</a></h2><p>AI tools (ChatGPT, Perplexity, and NotebookLM) were used:</p><ul><li><p>in writing portions of the workflow integration code,</p></li><li><p>to generate Matplotlib subplots and process flow diagrams, and</p></li><li><p>for proofreading and light revision to reduce potential publication errors.</p></li></ul><div><div class="flex flex-col w-full md:flex-row group/backmatter"><h2 id="acknowledgments" class="mt-5 text-base font-semibold group md:w-[200px] self-start md:flex-none opacity-90 group-hover/backmatter:opacity-100">Acknowledgments<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#acknowledgments" title="Link to Acknowledgments" aria-label="Link to Acknowledgments">¶</a></h2><div class="grow opacity-90 group-hover/backmatter:opacity-100 col-screen"><p>We gratefully acknowledge the contributions of the open source community — thank you to the giants on whose shoulders we stand.</p><p>This work was funded by FiOR Innovations and Woodburn Nursery &amp; Azaleas. We deeply appreciate their support and partnership.</p><p>Special thanks to Paniz Herrera, MBA, MSIST, for her invaluable guidance throughout this work.</p><p>We also thank Ryan Marinelli, PhD Fellow at the University of Oslo, for his assistance with proofreading and his insightful feedback.</p></div></div></div><section id="footnotes" class="article-left-grid subgrid-gap col-screen"><div><header class="text-lg font-semibold text-stone-900 dark:text-white group">Footnotes<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#footnotes" title="Link to Footnotes" aria-label="Link to Footnotes">¶</a></header></div><div class="pl-3 mb-8 text-xs text-stone-500 dark:text-stone-300"><ol><li id="fn-footnote-1" class="group"><div class="flex flex-row"><div class="break-words grow"><p>See <a target="_blank" rel="noreferrer" href="https://raw.githubusercontent.com/nickmccarty/scipy-2025/refs/heads/main/requirements.txt" class="">requirements.txt</a> for version details.</p></div><div class="flex flex-col grow-0"><a class="no-underline text-inherit hover:text-inherit p-1 select-none [@media(hover:hover)]:transition-opacity [@media(hover:hover)]:opacity-0 [@media(hover:hover)]:focus:opacity-100 [@media(hover:hover)]:group-hover:opacity-70" href="http://localhost:3000/#fnref-i3t457uC19" title="Link to Content" aria-label="Link to Content">↩</a></div></div></li></ol></div></section><section id="references" class="article-left-grid subgrid-gap col-screen"><div><button class="float-right p-1 px-2 text-xs border rounded hover:border-blue-500 dark:hover:border-blue-400">Collapse</button><header class="text-lg font-semibold text-stone-900 dark:text-white group">References<a class="no-underline text-inherit hover:text-inherit ml-2 select-none transition-opacity opacity-0 focus:opacity-100 group-hover:opacity-70" href="http://localhost:3000/#references" title="Link to References" aria-label="Link to References">¶</a></header></div><div class="pl-3 mb-8 text-xs text-stone-500 dark:text-stone-300"><ol><li class="break-words" id="cite-wu23">Wu, Q., &amp; Osco, L. P. (2023). samgeo: A Python package for segmenting geospatial data with the Segment Anything Model (SAM). <i>Journal of Open Source Software</i>, <i>8</i>(89), 5663. <a target="_blank" rel="noreferrer" href="https://doi.org/10.21105/joss.05663">10.21105/joss.05663</a></li><li class="break-words" id="cite-osco23">Osco, L. P., Wu, Q., de Lemos, E. L., Gonçalves, W. N., Ramos, A. P. M., Li, J., &amp; Junior, J. M. (2023). <i>The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot</i>. <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2306.16623">https://arxiv.org/abs/2306.16623</a></li><li class="break-words" id="cite-mayladan23">Mayladan, A., Nasrallah, H., Moughnieh, H., Shukor, M., &amp; Ghandour, A. J. (2024). <i>Zero-Shot Refinement of Buildings’ Segmentation Models using SAM</i>. <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2310.01845">https://arxiv.org/abs/2310.01845</a></li><li class="break-words" id="cite-kirillov23">Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., &amp; Girshick, R. (2023). <i>Segment Anything</i>. <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a></li><li class="break-words" id="cite-gdal">GDAL/OGR contributors. (2025). <i>GDAL/OGR Geospatial Data Abstraction software Library</i>. Open Source Geospatial Foundation. <a target="_blank" rel="noreferrer" href="https://doi.org/10.5281/zenodo.5884351">10.5281/zenodo.5884351</a></li><li class="break-words" id="cite-shapely">Gillies, S., van der Wel, C., Van den Bossche, J., Taves, M. W., Arnott, J., Ward, B. C., &amp; others. (2025). <i>Shapely (Version 2.1.1)</i>. <a target="_blank" rel="noreferrer" href="https://doi.org/10.5281/zenodo.5597138">10.5281/zenodo.5597138</a></li><li class="break-words" id="cite-geopandas">Jordahl, K., den Bossche, J. V., Fleischmann, M., Wasserman, J., McBride, J., Gerard, J., Tratner, J., Perry, M., Badaracco, A. G., Farmer, C., Hjelle, G. A., Snow, A. D., Cochran, M., Gillies, S., Culbertson, L., Bartos, M., Eubank, N., maxalbert, Bilogur, A., … Leblanc, F. (2020). <i>geopandas/geopandas: v0.8.1</i> (v0.8.1). Zenodo. <a target="_blank" rel="noreferrer" href="https://doi.org/10.5281/zenodo.3946761">10.5281/zenodo.3946761</a></li><li class="break-words" id="cite-rasterio">Gillies, S., &amp; others. (2013–). <i>Rasterio: geospatial raster I/O for Python programmers</i>. Mapbox. <a target="_blank" rel="noreferrer" href="https://github.com/rasterio/rasterio">https://github.com/rasterio/rasterio</a></li><li class="break-words" id="cite-matplotlib">Hunter, J. D. (2007). Matplotlib: A 2D graphics environment. <i>Computing in Science &amp; Engineering</i>, <i>9</i>(3), 90–95. <a target="_blank" rel="noreferrer" href="https://doi.org/10.1109/MCSE.2007.55">https://doi.org/10.1109/MCSE.2007.55</a></li><li class="break-words" id="cite-numpy">Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M., Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy. <i>Nature</i>, <i>585</i>(7825), 357–362. <a target="_blank" rel="noreferrer" href="https://doi.org/10.1038/s41586-020-2649-2">https://doi.org/10.1038/s41586-020-2649-2</a></li><li class="break-words" id="cite-opencv">Bradski, G. (2000). The OpenCV Library. <i>Dr. Dobb’s Journal of Software Tools</i>.</li><li class="break-words" id="cite-openpyxl">Gazoni, E., &amp; Clark, C. (2024). <i>OpenPyXL: A Python library to read/write Excel 2010 xlsx/xlsm/xltx/xltm files</i>. Python Package. <a target="_blank" rel="noreferrer" href="https://openpyxl.readthedocs.io/">https://openpyxl.readthedocs.io</a></li><li class="break-words" id="cite-pandas1">The Pandas Development Team. (2020). <i>pandas-dev/pandas: Pandas</i> (latest). Zenodo. <a target="_blank" rel="noreferrer" href="https://doi.org/10.5281/zenodo.3509134">10.5281/zenodo.3509134</a></li><li class="break-words" id="cite-pandas2">McKinney, W. (2010). Data Structures for Statistical Computing in Python. In Stéfan van der Walt &amp; Jarrod Millman (Eds.), <i>Proceedings of the 9th Python in Science Conference</i> (pp. 56–61). <a target="_blank" rel="noreferrer" href="https://doi.org/10.25080/Majora-92bf1922-00a">https://doi.org/10.25080/Majora-92bf1922-00a</a></li><li class="break-words" id="cite-pillow">Clark, A. (2015). <i>Pillow (PIL Fork) Documentation</i>. readthedocs. <a target="_blank" rel="noreferrer" href="https://buildmedia.readthedocs.org/media/pdf/pillow/latest/pillow.pdf">https://buildmedia.readthedocs.org/media/pdf/pillow/latest/pillow.pdf</a></li><li class="break-words" id="cite-pytorch">Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S. (2019). PyTorch: An Imperative Style, High‑Performance Deep Learning Library. <i>CoRR</i>, <i>abs/1912.01703</i>. <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1912.01703">https://arxiv.org/abs/1912.01703</a></li><li class="text-center list-none"><button class="p-2 border rounded hover:border-blue-500 dark:hover:border-blue-400">Collapse references</button></li></ol></div></section></main></article><script>((a,d)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let f=JSON.parse(sessionStorage.getItem(a)||"{}")[d||window.history.state.key];typeof f=="number"&&window.scrollTo(0,f)}catch(h){console.error(h),sessionStorage.removeItem(a)}})("positions", null)</script></body></html>